{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d04b8eab-ca24-4d8c-8bf6-60760761fbaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **USE CASE 1.** Comparative study of whether to consider the usage of FL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a1698f-d016-4400-9289-62816f1afdf3",
   "metadata": {},
   "source": [
    "In this use case we perform a comparative study to decide whether to consider or not the usage of FL in a given scenario.\n",
    "First, a centralized model is built to consider it as an upper-bound limit of performance. Such upper-bound limit is given by the fact that, when FL is a possibility, the data cannot be usually used in a centralized manner.\n",
    "Then, local models are built using only their local data and not sharing or communicating with the rest of clients. That would be the case of each client building isolated models without information sharing.\n",
    "Finally, federated models are built using their local data while not sharing it with any other client, but communicating so a model can be collaboratively built.\n",
    "\n",
    "This use case is implemented using TensorFlow and TensorFlow Federated (TFF), so the results are not biased by different architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ce5679-c24b-488b-8b1a-6c6d0a12b7db",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Centralized model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3bc140-7ad4-445f-99ba-c0a73afcd9fd",
   "metadata": {},
   "source": [
    "In this first section, it is shown how a centralized model using all available data is built using TensorFlow (which is the base library for TFF). More information about how to build TensorFlow models can be found at [TensorFlow's website](https://www.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e916ac05-79e0-4e17-b35e-b0e6d5a00002",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-15 17:38:10.223598: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-15 17:38:10.260850: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-15 17:38:10.262139: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-15 17:38:10.898843: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/jose/tff/tff_venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import collections\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.keras import models, layers, losses, metrics, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0095eac5-6342-4e90-9a0c-4bd3371b0ca4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Some parameters\n",
    "NUM_EPOCHS = 50 # Number of epochs (for fair comparison, is the number of epochs * number of rounds in federated scenario)\n",
    "BATCH_SIZE = 20 # Batch size for training phase\n",
    "SHUFFLE_BUFFER = 1000 # For shuffling data\n",
    "\n",
    "# Seeds for random numbers. The experiments are executed 10 times with different seeds\n",
    "seeds = [10] #, 20, 30, 40, 50, 60, 70, 80, 90, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e62951f0-f6b6-412c-8958-2a375e89f70b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Method to normalize image data\n",
    "def normalize_img(image, label):\n",
    "    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "    return tf.cast(image, tf.float32) / 255., label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2df866b-843c-4719-9a75-5d0365e28b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Method that creates a keras model with a CNN\n",
    "def create_keras_CNN():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, kernel_size=(5, 5), activation=\"relu\", padding=\"same\", strides=1),\n",
    "        layers.MaxPooling2D(pool_size=2, strides=2, padding='valid'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ])\n",
    "                \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d47b969-365e-456b-9042-5059db42b76a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-15 17:38:12.464470: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-05-15 17:38:12.464848: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-05-15 17:38:12.492997: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-05-15 17:38:12.524481: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [1]\n",
      "\t [[{{node Placeholder/_4}}]]\n",
      "2023-05-15 17:38:12.524887: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "/home/jose/tff/tff_venv/lib/python3.10/site-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 16s 5ms/step - loss: 0.1599 - sparse_categorical_accuracy: 0.9534\n",
      "Epoch 2/5\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 0.0593 - sparse_categorical_accuracy: 0.9826\n",
      "Epoch 3/5\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 0.0425 - sparse_categorical_accuracy: 0.9866\n",
      "Epoch 4/5\n",
      "3000/3000 [==============================] - 12s 4ms/step - loss: 0.0325 - sparse_categorical_accuracy: 0.9898\n",
      "Epoch 5/5\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 0.0249 - sparse_categorical_accuracy: 0.9922\n",
      "Training metrics: \n",
      "{'loss': [0.15989036858081818, 0.05932208150625229, 0.042547017335891724, 0.03249415010213852, 0.02490651048719883], 'sparse_categorical_accuracy': [0.9533500075340271, 0.9825666546821594, 0.9865666627883911, 0.9897500276565552, 0.9921666383743286]}\n",
      "\n",
      " 8/79 [==>...........................] - ETA: 0s - loss: 0.0633 - sparse_categorical_accuracy: 0.9795 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-15 17:39:19.114978: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_3' with dtype int64 and shape [1]\n",
      "\t [[{{node Placeholder/_3}}]]\n",
      "2023-05-15 17:39:19.115439: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [1]\n",
      "\t [[{{node Placeholder/_4}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 1s 7ms/step - loss: 0.0517 - sparse_categorical_accuracy: 0.9821\n",
      "Test metrics: \n",
      "{'loss': 0.051694318652153015, 'sparse_categorical_accuracy': 0.9821000099182129}\n",
      "\n",
      "Training and testing in 67.28 seconds\n"
     ]
    }
   ],
   "source": [
    "# Execute the experiment for each different seed\n",
    "for seed in seeds:\n",
    "    # Define the seed for random numbers\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "    # Load MNIST dataset from tfds\n",
    "    (mnist_train, mnist_test), mnist_info = tfds.load('mnist', split=['train', 'test'],\n",
    "                                                      shuffle_files=True, \n",
    "                                                      as_supervised=True, with_info=True)\n",
    "\n",
    "    # Normalize and prepare train data\n",
    "    ds_train = mnist_train.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    example_dataset = next(iter(ds_train))\n",
    "\n",
    "    ds_train = ds_train.cache()\n",
    "    ds_train = ds_train.shuffle(mnist_info.splits['train'].num_examples)\n",
    "    ds_train = ds_train.batch(BATCH_SIZE)\n",
    "    ds_train = ds_train.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Prepare test data\n",
    "    ds_test = mnist_test.map(\n",
    "        normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds_test = ds_test.batch(128)\n",
    "    ds_test = ds_test.cache()\n",
    "    ds_test = ds_test.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # Create and compile CNN model\n",
    "    keras_model = create_keras_CNN()\n",
    "\n",
    "    keras_model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "        loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[metrics.SparseCategoricalAccuracy()]\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Fit the model\n",
    "    train_metrics = keras_model.fit(\n",
    "        ds_train, epochs=NUM_EPOCHS, verbose=1,\n",
    "    )\n",
    "    train_metrics = train_metrics.history\n",
    "    print('Training metrics: ')\n",
    "    print(train_metrics)\n",
    "    print()\n",
    "\n",
    "    eval_metrics = keras_model.evaluate(ds_test, verbose=1, return_dict=True)\n",
    "    print('Test metrics: ')\n",
    "    print(eval_metrics)\n",
    "    print()\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print('Training and testing in {:.2f} seconds'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeb8637-421f-4ce1-a2b3-417d9ea978dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Local models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8889b6b7-aeb8-4d4a-ac6e-49853f5c2c94",
   "metadata": {},
   "source": [
    "In this section, we build local models using only their local data. The models are evaluated using a global test set, so all models are evaluated over the same data, which is also IID or non-IID, according to the characteristics of the data. Specifically, such global test set is comprised by all the client's test sets (the same procedure is also follow for the federated models, for a fair comparison). In this case, the models are also created in TensorFlow; more information about TensorFlow can be found at [TensorFlow's website](https://www.tensorflow.org/). TFF is used in this scenario to load the non-IID data if neccesary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6621451-ead0-4f11-8ee9-7d4aaf095b04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow_federated.python.simulation.datasets import emnist\n",
    "from tensorflow_federated.python.learning.algorithms import build_unweighted_fed_avg, build_fed_eval\n",
    "from tensorflow.keras import models, layers, losses, metrics, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f9d4f4a-d9e1-425f-9245-f1dca18e742d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Some parameters\n",
    "NUM_CLIENTS = 10 # Number of clients in the federated scenario\n",
    "NUM_ROUNDS = 10 # Number of learning rounds in the federated computation\n",
    "NUM_EPOCHS = 5 # Number of epochs that the local dataset is seen each round\n",
    "BATCH_SIZE = 20 # Batch size for training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e570b390-e3bf-4cb6-b7dc-65b994902878",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Indicates if IID data is used; if False, non-IID partitions are used\n",
    "is_iid = False\n",
    "\n",
    "# Seeds for random numbers. Execute the experiment several times.\n",
    "seeds = [10] #, 20, 30, 40, 50, 60, 70, 80, 90, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cb39312-f1cd-484f-9e98-6718aba90d7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Normalize the images\n",
    "def normalize_img(element):\n",
    "    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "    if is_iid:\n",
    "        image, label = element['image'], element['label']\n",
    "        image = tf.cast(image, tf.float32) / 255.\n",
    "    else:\n",
    "        image, label = element['pixels'], element['label']\n",
    "        image = tf.cast(image, tf.float32)\n",
    "    return image, label\n",
    "\n",
    "def normalize_test_niid(element):\n",
    "    image, label = element['image'], element['label']\n",
    "    image = tf.cast(image, tf.float32) / 255.\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "158aac6e-ad59-47a0-8128-77bd6bfa8fea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "def preprocess(dataset):\n",
    "    return dataset.shuffle(100, seed=seed).batch(BATCH_SIZE).map(normalize_img).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7689866d-c28a-4745-86e5-fc2a021359b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This method receives a client_id, and returns the training tf.data.Dataset for that client\n",
    "def create_tf_dataset_for_client_fn_train(client_id):\n",
    "    client_data = mnist_train_df[mnist_train_df['id'] == client_id].drop(columns='id')\n",
    "    return tf.data.Dataset.from_tensor_slices(client_data.to_dict('list'))\n",
    "\n",
    "# This method receives a client_id, and returns the testing tf.data.Dataset for that client\n",
    "def create_tf_dataset_for_client_fn_test(client_id):\n",
    "    client_data = mnist_test_df[mnist_test_df['id'] == client_id].drop(columns='id')\n",
    "    return tf.data.Dataset.from_tensor_slices(client_data.to_dict('list'))\n",
    "\n",
    "# Create a list of datasets (one for each client) from the complete dataset and the number of \n",
    "# clients (it will select the first client ids for simulation).\n",
    "def make_federated_data(client_data, n_clients):    \n",
    "    return [\n",
    "        preprocess(client_data.create_tf_dataset_for_client(x)) # Call previous preprocess method\n",
    "        for x in client_data.client_ids[0:n_clients]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "829b211c-90be-43e9-8a27-3196b29ca343",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Method that creates a keras model with a CNN\n",
    "def create_keras_CNN():\n",
    "    model = models.Sequential([\n",
    "        layers.Reshape((28, 28, 1), input_shape=(28, 28)),\n",
    "        layers.Conv2D(32, kernel_size=(5, 5), activation=\"relu\", padding=\"same\", strides=1),\n",
    "        layers.MaxPooling2D(pool_size=2, strides=2, padding='valid'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ])\n",
    "                \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94720007-c264-4416-93ad-76fa0591b931",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/tff/tff_venv/lib/python3.10/site-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 83ms/step - loss: 2.3983 - sparse_categorical_accuracy: 0.0645\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 2.2703 - sparse_categorical_accuracy: 0.1613\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 2.2375 - sparse_categorical_accuracy: 0.1398\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 2.1880 - sparse_categorical_accuracy: 0.2473\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.1355 - sparse_categorical_accuracy: 0.3333\n",
      "Training metrics: \n",
      "{'loss': [2.3983101844787598, 2.2703440189361572, 2.237501621246338, 2.1879615783691406, 2.1355299949645996], 'sparse_categorical_accuracy': [0.06451612710952759, 0.16129031777381897, 0.13978494703769684, 0.24731183052062988, 0.3333333432674408]}\n",
      "\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.2010 - sparse_categorical_accuracy: 0.3636\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.0638 - sparse_categorical_accuracy: 0.6154\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.3699 - sparse_categorical_accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.1672 - sparse_categorical_accuracy: 0.4167\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.1946 - sparse_categorical_accuracy: 0.4167\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.1697 - sparse_categorical_accuracy: 0.3333\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.2433 - sparse_categorical_accuracy: 0.0769\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.3083 - sparse_categorical_accuracy: 0.2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.2037 - sparse_categorical_accuracy: 0.3846\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.3298 - sparse_categorical_accuracy: 0.1818\n",
      "Clients test matrics: \n",
      "loss: 2.2251400470733644\t sparse_categorical_accuracy: 0.2989044331014156\n",
      "Training and testing in 1.36 seconds\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2.5647 - sparse_categorical_accuracy: 0.1009\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.3513 - sparse_categorical_accuracy: 0.0734\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2.2718 - sparse_categorical_accuracy: 0.1284\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.2071 - sparse_categorical_accuracy: 0.2202\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2.1620 - sparse_categorical_accuracy: 0.3670\n",
      "Training metrics: \n",
      "{'loss': [2.5646989345550537, 2.351294755935669, 2.2717952728271484, 2.207066535949707, 2.1620216369628906], 'sparse_categorical_accuracy': [0.10091742873191833, 0.07339449226856232, 0.12844036519527435, 0.22018349170684814, 0.3669724762439728]}\n",
      "\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 2.1353 - sparse_categorical_accuracy: 0.5455\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.1692 - sparse_categorical_accuracy: 0.4615\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.2924 - sparse_categorical_accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.1787 - sparse_categorical_accuracy: 0.1667\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.1916 - sparse_categorical_accuracy: 0.3333\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.1779 - sparse_categorical_accuracy: 0.3333\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.2043 - sparse_categorical_accuracy: 0.2308\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.2193 - sparse_categorical_accuracy: 0.2000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.1788 - sparse_categorical_accuracy: 0.2308\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.2144 - sparse_categorical_accuracy: 0.3636\n",
      "Clients test matrics: \n",
      "loss: 2.196182441711426\t sparse_categorical_accuracy: 0.28655012249946593\n",
      "Training and testing in 1.13 seconds\n",
      "Epoch 1/5\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 2.3895 - sparse_categorical_accuracy: 0.0959\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.2490 - sparse_categorical_accuracy: 0.1507\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.1882 - sparse_categorical_accuracy: 0.1644\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.1171 - sparse_categorical_accuracy: 0.2877\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.0099 - sparse_categorical_accuracy: 0.4658\n",
      "Training metrics: \n",
      "{'loss': [2.389517307281494, 2.2490241527557373, 2.188215970993042, 2.1171011924743652, 2.0098824501037598], 'sparse_categorical_accuracy': [0.09589041024446487, 0.15068493783473969, 0.16438356041908264, 0.2876712381839752, 0.465753436088562]}\n",
      "\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.3207 - sparse_categorical_accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.1449 - sparse_categorical_accuracy: 0.2308\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.2165 - sparse_categorical_accuracy: 0.3333\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.4985 - sparse_categorical_accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.2496 - sparse_categorical_accuracy: 0.1667\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.1390 - sparse_categorical_accuracy: 0.2500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.2755 - sparse_categorical_accuracy: 0.0769\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.2190 - sparse_categorical_accuracy: 0.2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.2399 - sparse_categorical_accuracy: 0.0769\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.3968 - sparse_categorical_accuracy: 0.0909\n",
      "Clients test matrics: \n",
      "loss: 2.2700278759002686\t sparse_categorical_accuracy: 0.142552450299263\n",
      "Training and testing in 1.10 seconds\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 2.3699 - sparse_categorical_accuracy: 0.0600\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 2.2670 - sparse_categorical_accuracy: 0.1300\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 2.2261 - sparse_categorical_accuracy: 0.2300\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.1765 - sparse_categorical_accuracy: 0.4100\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.1130 - sparse_categorical_accuracy: 0.4500\n",
      "Training metrics: \n",
      "{'loss': [2.369887113571167, 2.2669765949249268, 2.226139783859253, 2.176539659500122, 2.113037586212158], 'sparse_categorical_accuracy': [0.05999999865889549, 0.12999999523162842, 0.23000000417232513, 0.4099999964237213, 0.44999998807907104]}\n",
      "\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 2.2415 - sparse_categorical_accuracy: 0.1818\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.0588 - sparse_categorical_accuracy: 0.3846\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.3799 - sparse_categorical_accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.2648 - sparse_categorical_accuracy: 0.2500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.1421 - sparse_categorical_accuracy: 0.3333\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.1344 - sparse_categorical_accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.2131 - sparse_categorical_accuracy: 0.2308\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.4395 - sparse_categorical_accuracy: 0.1000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.2159 - sparse_categorical_accuracy: 0.2308\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.3157 - sparse_categorical_accuracy: 0.2727\n",
      "Clients test matrics: \n",
      "loss: 2.2405677318572996\t sparse_categorical_accuracy: 0.24840326681733133\n",
      "Training and testing in 1.16 seconds\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 2.4822 - sparse_categorical_accuracy: 0.0762\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.2499 - sparse_categorical_accuracy: 0.1524\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.1830 - sparse_categorical_accuracy: 0.2952\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2.1039 - sparse_categorical_accuracy: 0.4762\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2.0210 - sparse_categorical_accuracy: 0.3048\n",
      "Training metrics: \n",
      "{'loss': [2.4821808338165283, 2.2499265670776367, 2.1829559803009033, 2.103895664215088, 2.0210344791412354], 'sparse_categorical_accuracy': [0.07619047909975052, 0.15238095819950104, 0.29523810744285583, 0.4761904776096344, 0.3047619163990021]}\n",
      "\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 2.1992 - sparse_categorical_accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.0788 - sparse_categorical_accuracy: 0.4615\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.0738 - sparse_categorical_accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.1091 - sparse_categorical_accuracy: 0.2500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.0076 - sparse_categorical_accuracy: 0.5833\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.0821 - sparse_categorical_accuracy: 0.3333\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.0894 - sparse_categorical_accuracy: 0.2308\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.0362 - sparse_categorical_accuracy: 0.6000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.2105 - sparse_categorical_accuracy: 0.2308\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.0573 - sparse_categorical_accuracy: 0.5455\n",
      "Clients test matrics: \n",
      "loss: 2.0943933486938477\t sparse_categorical_accuracy: 0.37907537519931794\n",
      "Training and testing in 1.24 seconds\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.4901 - sparse_categorical_accuracy: 0.0874\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.2899 - sparse_categorical_accuracy: 0.1165\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.2276 - sparse_categorical_accuracy: 0.1262\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.1352 - sparse_categorical_accuracy: 0.2621\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.0545 - sparse_categorical_accuracy: 0.2136\n",
      "Training metrics: \n",
      "{'loss': [2.4900963306427, 2.289942979812622, 2.227590799331665, 2.1351656913757324, 2.0544981956481934], 'sparse_categorical_accuracy': [0.08737864345312119, 0.11650485545396805, 0.12621359527111053, 0.26213592290878296, 0.21359223127365112]}\n",
      "\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2.3040 - sparse_categorical_accuracy: 0.1818\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.1319 - sparse_categorical_accuracy: 0.3077\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.1744 - sparse_categorical_accuracy: 0.2222\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.2272 - sparse_categorical_accuracy: 0.1667\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.1864 - sparse_categorical_accuracy: 0.1667\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.2182 - sparse_categorical_accuracy: 0.1667\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.2434 - sparse_categorical_accuracy: 0.0769\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.9970 - sparse_categorical_accuracy: 0.4000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.2567 - sparse_categorical_accuracy: 0.1538\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.2044 - sparse_categorical_accuracy: 0.1818\n",
      "Clients test matrics: \n",
      "loss: 2.1943588733673094\t sparse_categorical_accuracy: 0.20243201777338982\n",
      "Training and testing in 1.08 seconds\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.4450 - sparse_categorical_accuracy: 0.0841\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.2416 - sparse_categorical_accuracy: 0.2710\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.1891 - sparse_categorical_accuracy: 0.2430\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.0827 - sparse_categorical_accuracy: 0.4673\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 1.9621 - sparse_categorical_accuracy: 0.5794\n",
      "Training metrics: \n",
      "{'loss': [2.444978713989258, 2.241567850112915, 2.189136505126953, 2.082712411880493, 1.96206796169281], 'sparse_categorical_accuracy': [0.08411215245723724, 0.2710280418395996, 0.2429906576871872, 0.4672897160053253, 0.5794392228126526]}\n",
      "\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.1855 - sparse_categorical_accuracy: 0.0909\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.1527 - sparse_categorical_accuracy: 0.2308\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.9948 - sparse_categorical_accuracy: 0.5556\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.1426 - sparse_categorical_accuracy: 0.1667\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.1008 - sparse_categorical_accuracy: 0.3333\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.9456 - sparse_categorical_accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.9760 - sparse_categorical_accuracy: 0.7692\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.9547 - sparse_categorical_accuracy: 0.5000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.1848 - sparse_categorical_accuracy: 0.1538\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.1281 - sparse_categorical_accuracy: 0.5455\n",
      "Clients test matrics: \n",
      "loss: 2.0765658140182497\t sparse_categorical_accuracy: 0.40124321132898333\n",
      "Training and testing in 1.12 seconds\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 2.4522 - sparse_categorical_accuracy: 0.0824\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 2.3126 - sparse_categorical_accuracy: 0.1412\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 2.2251 - sparse_categorical_accuracy: 0.2941\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.1330 - sparse_categorical_accuracy: 0.3176\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 2.0559 - sparse_categorical_accuracy: 0.4824\n",
      "Training metrics: \n",
      "{'loss': [2.4522476196289062, 2.312572479248047, 2.22509503364563, 2.132991075515747, 2.0558695793151855], 'sparse_categorical_accuracy': [0.08235294371843338, 0.1411764770746231, 0.29411765933036804, 0.3176470696926117, 0.48235294222831726]}\n",
      "\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.2120 - sparse_categorical_accuracy: 0.2727\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.3482 - sparse_categorical_accuracy: 0.2308\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.2668 - sparse_categorical_accuracy: 0.3333\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.3319 - sparse_categorical_accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.2751 - sparse_categorical_accuracy: 0.3333\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.0443 - sparse_categorical_accuracy: 0.5833\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.2287 - sparse_categorical_accuracy: 0.3077\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.0123 - sparse_categorical_accuracy: 0.6000\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.2289 - sparse_categorical_accuracy: 0.1538\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.2577 - sparse_categorical_accuracy: 0.3636\n",
      "Clients test matrics: \n",
      "loss: 2.2205920219421387\t sparse_categorical_accuracy: 0.3178671389818192\n",
      "Training and testing in 0.96 seconds\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 1s 7ms/step - loss: 2.4230 - sparse_categorical_accuracy: 0.0826\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 2.3538 - sparse_categorical_accuracy: 0.1193\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.2624 - sparse_categorical_accuracy: 0.1284\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2.1989 - sparse_categorical_accuracy: 0.2202\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 2.1500 - sparse_categorical_accuracy: 0.2844\n",
      "Training metrics: \n",
      "{'loss': [2.423037528991699, 2.353820323944092, 2.26236891746521, 2.1988511085510254, 2.1500229835510254], 'sparse_categorical_accuracy': [0.08256880939006805, 0.11926605552434921, 0.12844036519527435, 0.22018349170684814, 0.2844036817550659]}\n",
      "\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.2703 - sparse_categorical_accuracy: 0.1818\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.1528 - sparse_categorical_accuracy: 0.2308\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.1932 - sparse_categorical_accuracy: 0.2222\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.1744 - sparse_categorical_accuracy: 0.3333\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.1821 - sparse_categorical_accuracy: 0.2500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.1455 - sparse_categorical_accuracy: 0.3333\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.2711 - sparse_categorical_accuracy: 0.0769\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.2194 - sparse_categorical_accuracy: 0.2000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.1954 - sparse_categorical_accuracy: 0.2308\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.2150 - sparse_categorical_accuracy: 0.1818\n",
      "Clients test matrics: \n",
      "loss: 2.2019356966018675\t sparse_categorical_accuracy: 0.22409868314862252\n",
      "Training and testing in 1.32 seconds\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 2.4234 - sparse_categorical_accuracy: 0.1136\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 2.2192 - sparse_categorical_accuracy: 0.2841\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 2.1430 - sparse_categorical_accuracy: 0.3409\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 2.0319 - sparse_categorical_accuracy: 0.4545\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.9065 - sparse_categorical_accuracy: 0.3977\n",
      "Training metrics: \n",
      "{'loss': [2.4233651161193848, 2.2192323207855225, 2.143000602722168, 2.0318922996520996, 1.9065468311309814], 'sparse_categorical_accuracy': [0.11363636702299118, 0.28409090638160706, 0.34090909361839294, 0.4545454680919647, 0.39772728085517883]}\n",
      "\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.4515 - sparse_categorical_accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.1153 - sparse_categorical_accuracy: 0.3077\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.2590 - sparse_categorical_accuracy: 0.1111\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.4777 - sparse_categorical_accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.1705 - sparse_categorical_accuracy: 0.2500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.2011 - sparse_categorical_accuracy: 0.1667\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.3050 - sparse_categorical_accuracy: 0.0769\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.3888 - sparse_categorical_accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.2809 - sparse_categorical_accuracy: 0.1538\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.0297 - sparse_categorical_accuracy: 0.1818\n",
      "Clients test matrics: \n",
      "loss: 2.26794490814209\t sparse_categorical_accuracy: 0.12480575293302536\n",
      "Training and testing in 1.02 seconds\n"
     ]
    }
   ],
   "source": [
    "# Run experiment for each different seed\n",
    "for seed in seeds:\n",
    "    # Define the seed for random numbers\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "    # Load either IID or non-IID data\n",
    "    if not is_iid:\n",
    "        # Load federated version of mnist from TFF (== EMNIST loading only the digits)\n",
    "        mnist_train, mnist_test = emnist.load_data(only_digits=True)\n",
    "    else:\n",
    "        # Load MNIST from tfds, and get train and test partitions\n",
    "        mnist = tfds.load('mnist')\n",
    "        mnist_train, mnist_test = mnist['train'], mnist['test']\n",
    "\n",
    "        # Transform the data to a dataframe\n",
    "        mnist_train_df = tfds.as_dataframe(mnist_train)\n",
    "\n",
    "        # Create a random list of ids. Each instance is given a random id meaning the client where will be distributed\n",
    "        ids_train = [i for i in range(NUM_CLIENTS) for _ in range(len(mnist_train)//NUM_CLIENTS)]\n",
    "        random.Random(seed).shuffle(ids_train)\n",
    "        # Add the id assignment to the dataframe\n",
    "        mnist_train_df['id'] = ids_train\n",
    "\n",
    "        # Do the same with the test data\n",
    "        mnist_test_df = tfds.as_dataframe(mnist_test)\n",
    "        ids_test = [i for i in range(NUM_CLIENTS) for _ in range(len(mnist_test)//NUM_CLIENTS)]\n",
    "        random.Random(seed+1).shuffle(ids_test)\n",
    "        mnist_test_df['id'] = ids_test\n",
    "\n",
    "        # Get traning and testing datasets for each client\n",
    "        mnist_train = tff.simulation.datasets.ClientData.from_clients_and_tf_fn(\n",
    "            client_ids=list(range(0,NUM_CLIENTS)),\n",
    "            serializable_dataset_fn=create_tf_dataset_for_client_fn_train\n",
    "        )\n",
    "        mnist_test = tff.simulation.datasets.ClientData.from_clients_and_tf_fn(\n",
    "            client_ids=list(range(0,NUM_CLIENTS)),\n",
    "            serializable_dataset_fn=create_tf_dataset_for_client_fn_test\n",
    "        )\n",
    "\n",
    "    # Create the federated train data from the full mnist_train data, and filtering only \n",
    "    # NUM_CLIENTS clients\n",
    "    train_data = make_federated_data(mnist_train, NUM_CLIENTS)\n",
    "    test_data = make_federated_data(mnist_test, NUM_CLIENTS)\n",
    "\n",
    "    # Build model for each client\n",
    "    for client in range(NUM_CLIENTS):\n",
    "        keras_model = create_keras_CNN()\n",
    "\n",
    "        keras_model.compile(\n",
    "            optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "            loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=[metrics.SparseCategoricalAccuracy()]\n",
    "        )\n",
    "\n",
    "        start_time = time.time()\n",
    "            \n",
    "        # Train model\n",
    "        train_metrics = keras_model.fit(\n",
    "            train_data[client], epochs=NUM_EPOCHS, verbose=1,\n",
    "        )\n",
    "        \n",
    "        train_metrics = train_metrics.history\n",
    "        print('Training metrics: ')\n",
    "        print(train_metrics)\n",
    "        print()\n",
    "        \n",
    "        # Evaluate the model over global test data\n",
    "        # i.e., for ease of simulation, evaluate over each client's test set and average\n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "        for test_client in range(NUM_CLIENTS):\n",
    "            metrics_test = keras_model.evaluate(test_data[test_client], verbose=1,\n",
    "                                                return_dict=True)\n",
    "            test_loss += metrics_test['loss']\n",
    "            test_acc += metrics_test['sparse_categorical_accuracy']\n",
    "                                                \n",
    "        test_loss /= NUM_CLIENTS\n",
    "        test_acc /= NUM_CLIENTS\n",
    "                                                \n",
    "        print('Clients test matrics: ')\n",
    "        print(f\"loss: {test_loss}\\t sparse_categorical_accuracy: {test_acc}\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "    \n",
    "        print('Training and testing in {:.2f} seconds'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca058ee-df2f-4d53-b065-915aaa381606",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Federated models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfad7595-0fd6-41e1-b69e-bd86a7ace18e",
   "metadata": {},
   "source": [
    "In this section we build a shared global model between all clients using FL. Each client updates the model using its local data and send the updates to the central server, that orchestrates the global learning. All the models are evaluated simulating a global test set, comprised of each client's test data (as previously done with the local models). More detailed information and analysis about how to build FL models is provided in subsequent use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a576f880-2e0e-4754-a351-d6f99d25aa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow_federated.python.simulation.datasets import emnist\n",
    "from tensorflow_federated.python.learning.algorithms import build_unweighted_fed_avg, build_fed_eval\n",
    "from tensorflow.keras import models, layers, losses, metrics, optimizers\n",
    "\n",
    "from tensorflow_federated.python.learning.model_update_aggregator import dp_aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "892e35ee-9192-46e4-ae89-1a73f0f7f834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Some parameters\n",
    "NUM_CLIENTS = 10 # Number of clients in the federated scenario\n",
    "NUM_ROUNDS = 10 # Number of learning rounds in the federated computation\n",
    "NUM_EPOCHS = 5 # Number of epochs that the local dataset is seen each round\n",
    "BATCH_SIZE = 20 # Batch size for training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88fae577-6bed-4439-a3e1-70901a6e8d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicates if IID data is used; if False, non-IID partitions are used\n",
    "is_iid = False\n",
    "\n",
    "# Seeds for random numbers. Execute the experiment several times.\n",
    "seeds = [10] #, 20, 30, 40, 50, 60, 70, 80, 90, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05d0cb6c-992c-46a4-9ce0-2fbc5be0a354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess federated data\n",
    "def preprocess(dataset):\n",
    "    def batch_format_fn(element):\n",
    "        if is_iid:\n",
    "            return collections.OrderedDict(\n",
    "                x=element['image']/255.0,\n",
    "                y=element['label']\n",
    "            )\n",
    "        else:\n",
    "            return collections.OrderedDict(\n",
    "                x=element['pixels'],\n",
    "                y=element['label']\n",
    "            )\n",
    "\n",
    "    return dataset.repeat(NUM_EPOCHS).shuffle(100, seed=seed).batch(BATCH_SIZE).map(batch_format_fn)\n",
    "\n",
    "\n",
    "# Construct a list of datasets (one for each client) from the complete dataset and the number of \n",
    "# clients (it will select the first client ids for simulation).\n",
    "def make_federated_data(client_data, n_clients): \n",
    "    return [\n",
    "        preprocess(client_data.create_tf_dataset_for_client(x)) # Call previous preprocess method\n",
    "        for x in client_data.client_ids[0:n_clients]\n",
    "    ]\n",
    "\n",
    "\n",
    "# Create testing federated dataset\n",
    "# If t is none, each client receives its testing data\n",
    "# If t is a number, the t-th testing data is given to all clients\n",
    "def make_federated_data_test(client_data, n_clients, t=None): \n",
    "    if t is None:\n",
    "        # Return the test data of each client\n",
    "        return [\n",
    "            preprocess(client_data.create_tf_dataset_for_client(x)) \n",
    "            for x in client_data.client_ids[0:n_clients]\n",
    "        ]\n",
    "    else:\n",
    "        # Return the test data of a given client t\n",
    "        return [\n",
    "            preprocess(client_data.create_tf_dataset_for_client(x)) \n",
    "            for x in [client_data.client_ids[t]]*n_clients\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c40ee1b5-f770-4d56-a353-3d26e310afdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method that creates a keras model with a CNN\n",
    "def create_keras_CNN():\n",
    "    model = models.Sequential([\n",
    "        layers.Reshape((28, 28, 1), input_shape=(28, 28)),\n",
    "        layers.Conv2D(32, kernel_size=(5, 5), activation=\"relu\", padding=\"same\", strides=1),\n",
    "        layers.MaxPooling2D(pool_size=2, strides=2, padding='valid'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ])\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "def model_fn():\n",
    "    keras_model = create_keras_CNN()\n",
    "    \n",
    "    return tff.learning.models.from_keras_model(\n",
    "        keras_model,\n",
    "        input_spec=train_data[0].element_spec,\n",
    "        loss=losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[metrics.SparseCategoricalAccuracy()]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c4cb273-3eb4-4686-9fc9-592a98ee5430",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-15 17:39:33.984369: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'batch_input' with dtype float and shape [?,28,28]\n",
      "\t [[{{node batch_input}}]]\n",
      "2023-05-15 17:39:34.297456: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'args_1' with dtype float and shape [?,28,28]\n",
      "\t [[{{node args_1}}]]\n",
      "2023-05-15 17:39:34.366979: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3' with dtype float and shape [?,14,14,32]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_3}}]]\n",
      "2023-05-15 17:39:34.367078: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_5' with dtype float and shape [?,28,28,1]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_5}}]]\n",
      "2023-05-15 17:39:34.367132: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_7' with dtype float and shape [?,28,28]\n",
      "\t [[{{node gradients/StatefulPartitionedCall_grad/StatefulPartitionedCall_7}}]]\n",
      "2023-05-15 17:39:35.822799: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-05-15 17:39:35.822886: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-05-15 17:39:35.841653: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-05-15 17:39:35.841711: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-05-15 17:39:38.393983: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-05-15 17:39:38.394107: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-05-15 17:39:38.569273: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-05-15 17:39:38.571770: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-05-15 17:39:38.585081: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-05-15 17:39:38.585204: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-05-15 17:39:38.599935: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-05-15 17:39:38.600094: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-05-15 17:39:38.620707: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-05-15 17:39:38.620917: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-05-15 17:39:38.643177: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-05-15 17:39:38.643238: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-05-15 17:39:38.650291: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-05-15 17:39:38.650435: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-05-15 17:39:38.660810: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-05-15 17:39:38.660892: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-05-15 17:39:38.669566: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-05-15 17:39:38.669745: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-05-15 17:39:38.673750: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-05-15 17:39:38.673877: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-05-15 17:39:38.676839: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-05-15 17:39:38.676887: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-05-15 17:39:38.682173: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-05-15 17:39:38.682238: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-05-15 17:39:38.689598: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-05-15 17:39:38.689774: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-05-15 17:39:38.718667: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-05-15 17:39:38.718828: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 10,  \t Loss=0.4858, \t Accuracy=0.8947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-15 17:39:50.585441: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'batch_input' with dtype float and shape [?,28,28]\n",
      "\t [[{{node batch_input}}]]\n",
      "2023-05-15 17:39:50.653936: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'args_1' with dtype float and shape [?,28,28]\n",
      "\t [[{{node args_1}}]]\n",
      "2023-05-15 17:39:50.921355: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-05-15 17:39:50.921428: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-05-15 17:39:50.934130: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-05-15 17:39:50.934180: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-05-15 17:39:50.956963: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-05-15 17:39:50.957045: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-05-15 17:39:50.981337: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-05-15 17:39:50.981431: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-05-15 17:39:51.501952: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-05-15 17:39:51.502164: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-05-15 17:39:51.546519: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-05-15 17:39:51.546601: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-05-15 17:39:51.553315: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-05-15 17:39:51.553420: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-05-15 17:39:51.561451: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-05-15 17:39:51.561513: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-05-15 17:39:51.572434: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-05-15 17:39:51.572542: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-05-15 17:39:51.586521: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-05-15 17:39:51.586590: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-05-15 17:39:51.608832: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2023-05-15 17:39:51.608888: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data, \t Loss=1.4620, \t Accuracy=0.3636\n",
      "Test data, \t Loss=0.5058, \t Accuracy=0.8462\n",
      "Test data, \t Loss=0.9490, \t Accuracy=0.7778\n",
      "Test data, \t Loss=1.0102, \t Accuracy=0.5000\n",
      "Test data, \t Loss=0.2844, \t Accuracy=1.0000\n",
      "Test data, \t Loss=0.5500, \t Accuracy=0.7500\n",
      "Test data, \t Loss=0.5408, \t Accuracy=0.7692\n",
      "Test data, \t Loss=0.3717, \t Accuracy=0.9000\n",
      "Test data, \t Loss=1.0282, \t Accuracy=0.5385\n",
      "Test data, \t Loss=1.1412, \t Accuracy=0.5455\n",
      "\n",
      "Mean test (global), \t Loss=0.7843, \t Accuracy=0.6991\n",
      "Training and testing in 21.19 seconds\n"
     ]
    }
   ],
   "source": [
    "# Run experiment for each different seed\n",
    "for seed in seeds:\n",
    "    # Define the seed for random numbers\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "    \n",
    "    # Load either IID or non-IID data\n",
    "    if not is_iid:\n",
    "        # Load federated version of mnist from TFF (== EMNIST loading only the digits)\n",
    "        mnist_train, mnist_test = emnist.load_data(only_digits=True)\n",
    "    else:\n",
    "        # Load MNIST from tfds, and get train and test partitions\n",
    "        mnist = tfds.load('mnist')\n",
    "        mnist_train, mnist_test = mnist['train'], mnist['test']\n",
    "\n",
    "        # Transform the data to a dataframe\n",
    "        mnist_train_df = tfds.as_dataframe(mnist_train)\n",
    "\n",
    "        # Create a random list of ids. Each instance is given a random id meaning the client where will be distributed\n",
    "        ids_train = [i for i in range(NUM_CLIENTS) for _ in range(len(mnist_train)//NUM_CLIENTS)]\n",
    "        random.Random(seed).shuffle(ids_train)\n",
    "        # Add the id assignment to the dataframe\n",
    "        mnist_train_df['id'] = ids_train\n",
    "\n",
    "        # Do the same with the test data\n",
    "        mnist_test_df = tfds.as_dataframe(mnist_test)\n",
    "        ids_test = [i for i in range(NUM_CLIENTS) for _ in range(len(mnist_test)//NUM_CLIENTS)]\n",
    "        random.Random(seed+1).shuffle(ids_test)\n",
    "        mnist_test_df['id'] = ids_test\n",
    "\n",
    "        # Get traning datasets for each client\n",
    "        mnist_train = tff.simulation.datasets.ClientData.from_clients_and_tf_fn(\n",
    "            client_ids=list(range(0,NUM_CLIENTS)),\n",
    "            serializable_dataset_fn=create_tf_dataset_for_client_fn_train\n",
    "        )\n",
    "\n",
    "    # Create the federated train data from the full mnist_train data, and filtering only \n",
    "    # NUM_CLIENTS clients\n",
    "    train_data = make_federated_data(mnist_train, NUM_CLIENTS)\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    # Define training strategy\n",
    "    training_process = build_unweighted_fed_avg(\n",
    "        model_fn,\n",
    "        client_optimizer_fn=lambda: optimizers.Adam(learning_rate=0.001),\n",
    "        server_optimizer_fn=lambda: optimizers.Adam(learning_rate=0.01),\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    train_state = training_process.initialize()\n",
    "\n",
    "    for round_num in range(1, NUM_ROUNDS+1):\n",
    "        # Train next round (send model to clients, local training, and server model averaging)\n",
    "        result = training_process.next(train_state, train_data)\n",
    "\n",
    "        # Current state of the model\n",
    "        train_state = result.state\n",
    "\n",
    "        # Get and print metrics, as the loss and accuracy (averaged across all clients)\n",
    "        train_metrics = result.metrics['client_work']['train']\n",
    "\n",
    "        if round_num == NUM_ROUNDS:\n",
    "            print('Round {:2d},  \\t Loss={:.4f}, \\t Accuracy={:.4f}'.format(round_num, train_metrics['loss'],\n",
    "                                                                            train_metrics['sparse_categorical_accuracy']))\n",
    "\n",
    "    # Indicate that the model arquitecture is the one proposed before\n",
    "    evaluation_process = build_fed_eval(model_fn)\n",
    "\n",
    "    # Initialize the process and set the weights to those previously trained (getting from the training\n",
    "    # state and setting to the evaluation one).\n",
    "    evaluation_state = evaluation_process.initialize()\n",
    "    model_weights = training_process.get_model_weights(train_state)\n",
    "    evaluation_state = evaluation_process.set_model_weights(evaluation_state, model_weights)\n",
    "\n",
    "    # Average loss and accuracy\n",
    "    losses_clients = []\n",
    "    accuracies_clients = []\n",
    "    \n",
    "     # Evaluate on the test set of each client\n",
    "    for i in range(NUM_CLIENTS):\n",
    "        test_data = make_federated_data_test(mnist_test, NUM_CLIENTS, t=i) # data of i-th client\n",
    "\n",
    "        # Pass test data to the model in each client\n",
    "        evaluation_output = evaluation_process.next(evaluation_state, test_data)\n",
    "\n",
    "        # Get and print metrics\n",
    "        eval_metrics = evaluation_output.metrics['client_work']['eval']['current_round_metrics']\n",
    "        print('Test data, \\t Loss={:.4f}, \\t Accuracy={:.4f}'.format(eval_metrics['loss'], \n",
    "                                                                     eval_metrics['sparse_categorical_accuracy']))\n",
    "\n",
    "        losses_clients.append(eval_metrics['loss'])\n",
    "        accuracies_clients.append(eval_metrics['sparse_categorical_accuracy'])\n",
    "\n",
    "    # Print mean metrics over all test sets\n",
    "    print()\n",
    "    print('Mean test (global), \\t Loss={:.4f}, \\t Accuracy={:.4f}'.format(np.mean(losses_clients), np.mean(accuracies_clients)))\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print('Training and testing in {:.2f} seconds'.format(end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
