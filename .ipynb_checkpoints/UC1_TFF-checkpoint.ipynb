{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d85c969a-bbe5-4374-879d-ef8ad7c4f690",
   "metadata": {},
   "source": [
    "# **USE CASE 1.** Image classification in TFF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e1c316-db26-45e6-add8-2604b6dc6b11",
   "metadata": {},
   "source": [
    "## Required libraries and configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4151af53-0807-4859-8478-1e7d6b0ec26a",
   "metadata": {},
   "source": [
    "Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b000fad2-9ca1-425c-96e7-ab21b62bed47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 12:23:07.833824: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-23 12:23:09.467954: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-23 12:23:09.468528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-23 12:23:09.468540: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/jose/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow_federated.python.simulation.datasets import emnist\n",
    "from tensorflow_federated.python.learning.algorithms import build_unweighted_fed_avg, build_fed_eval\n",
    "from tensorflow.keras import models, layers, losses, metrics, optimizers\n",
    "\n",
    "# Option for debugging warning errors\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f499cef0-0261-47a1-b2a7-e2819e1d1006",
   "metadata": {},
   "source": [
    "Define some parameters for the simulation, such as the number of clients in the federated scenario, the number of federated rounds, the number of epochs of each client before communicating, and the batch size for training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a676bb7f-845a-4ca3-9b17-d33331dbbd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some parameters\n",
    "NUM_CLIENTS = 10 # Number of clients in the federated scenario\n",
    "NUM_ROUNDS = 10 # Number of learning rounds in the federated computation\n",
    "NUM_EPOCHS = 5 # Number of epochs that the local dataset is seen each round\n",
    "BATCH_SIZE = 20 # Batch size for training phase\n",
    "\n",
    "# Define the seed for random numbers\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.keras.utils.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96891704-dc2a-443e-a67c-9c48c631f8ed",
   "metadata": {},
   "source": [
    "## Loading and preparing the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28bf10d-edd3-4e9a-b5d9-8632dc5f07b9",
   "metadata": {},
   "source": [
    "TFF provides the EMNIST dataset (which includes both digits and letters) in its federated version, i.e., each client in the federated scenario has the digits/characters that were written by a single person. This approach would be closer to a real problem where the data is **non-i.i.d.** distributed, and the following code cell includes (commented) how to load the federated version of MNIST dataset from TFF. If the user prefers, that line may be uncommented, while the next two cells where the i.i.d. data loading is performed, should be skipped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c7b1159-5065-4288-b000-52a39819f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load federated version of mnist from TFF (== EMNIST loading only the digits)\n",
    "# Uncomment next line to load the federated MNIST\n",
    "# mnist_train, mnist_test = emnist.load_data(only_digits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd54619-7beb-48e6-9665-c8082dba3b59",
   "metadata": {},
   "source": [
    "However, in this notebook we aim to also show how to load other datasets different from those available in TFF, and how to distribute among the different users for simulation purposes. This distribution is made as **i.i.d.** For non-i.i.d. distribution, skip next two cells and uncomment the previous one.\n",
    "\n",
    "In this case, we load the MNIST dataset from standard tensorflow (both training and testing partitions).\n",
    "Later, each instance is distributed to a different (and randomly chosen) client. The distribution is performed the same for training and testing data. As seen in the code, once retrieved the dataset from tensorflow, it is transformed to a dataframe; it will ease the conversion to a TFF ClientData object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f7f6adb-c371-4be9-8dd8-2df888922265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST from tfds, and get train and test partitions\n",
    "mnist = tfds.load('mnist')\n",
    "mnist_train, mnist_test = mnist['train'], mnist['test']\n",
    "\n",
    "# Transform the data to a dataframe\n",
    "mnist_train_df = tfds.as_dataframe(mnist_train)\n",
    "\n",
    "# Create a random list of ids. Each instance is given a random id meaning the client where will be distributed\n",
    "ids_train = [i for i in range(NUM_CLIENTS) for _ in range(len(mnist_train)//NUM_CLIENTS)]\n",
    "random.Random(seed).shuffle(ids_train)\n",
    "# Add the id assignment to the dataframe\n",
    "mnist_train_df['id'] = ids_train\n",
    "\n",
    "# Do the same with the test data\n",
    "mnist_test_df = tfds.as_dataframe(mnist_test)\n",
    "ids_test = [i for i in range(NUM_CLIENTS) for _ in range(len(mnist_test)//NUM_CLIENTS)]\n",
    "random.Random(seed+1).shuffle(ids_test)\n",
    "mnist_test_df['id'] = ids_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d50d7d92-d0f2-4dfb-b792-b4228710772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method receives a client_id, and returns the training tf.data.Dataset for that client\n",
    "def create_tf_dataset_for_client_fn_train(client_id):\n",
    "    client_data = mnist_train_df[mnist_train_df['id'] == client_id].drop(columns='id')\n",
    "    return tf.data.Dataset.from_tensor_slices(client_data.to_dict('list'))\n",
    "\n",
    "# This method receives a client_id, and returns the testing tf.data.Dataset for that client\n",
    "def create_tf_dataset_for_client_fn_test(client_id):\n",
    "    client_data = mnist_test_df[mnist_test_df['id'] == client_id].drop(columns='id')\n",
    "    return tf.data.Dataset.from_tensor_slices(client_data.to_dict('list'))\n",
    "\n",
    "mnist_train = tff.simulation.datasets.ClientData.from_clients_and_tf_fn(\n",
    "    client_ids=list(range(0,NUM_CLIENTS)),\n",
    "    serializable_dataset_fn=create_tf_dataset_for_client_fn_train\n",
    ")\n",
    "mnist_test = tff.simulation.datasets.ClientData.from_clients_and_tf_fn(\n",
    "    client_ids=list(range(0,NUM_CLIENTS)),\n",
    "    serializable_dataset_fn=create_tf_dataset_for_client_fn_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262d32ca-8134-49bb-af69-678b529037bf",
   "metadata": {},
   "source": [
    "Show the structure of the data. It includes two tags: 'label' containing the class label of each example, and 'image' (or 'pixels', if using the non-i.i.d. data from TFF) including the images in 28*28 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "facda5b8-19ce-43a3-949c-2983e2ea34dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None),\n",
       " 'label': TensorSpec(shape=(), dtype=tf.int32, name=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train.element_type_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51041223-0be5-4d00-ab99-95ee245f4fcf",
   "metadata": {},
   "source": [
    "Either if you are finally using the non-i.i.d. or the i.i.d. dataset, you should resume the execution in the next cell, since such preprocessing is performed for both cases. The only difference is that, in the i.i.d. data extracted from tensorflow, the input attributes are referred as 'image', while in the non-i.i.d. dataset from TFF, it is referred as 'pixels'. You should change it if neccessary in next cell.\n",
    "\n",
    "Here, we create and prepare the federated dataset.\n",
    " * The elements are distributed to the clients by id, which describes the user who wrote each digit.\n",
    " * The dataset is converted into an OrderedDict structure, where the images are referred as *x* and the labels as *y*.\n",
    " * The data is shuffled, organized in batches, and the `repeat` statement helps to run the dataset over a number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e255d668-d0c3-44f8-a662-44d8dbc1e7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "    def batch_format_fn(element):\n",
    "        return collections.OrderedDict(\n",
    "            x=element['image']/255, # If using the non-i.i.d. data from TFF; change that line by: x = element['pixels'] (and do not divide by 255)\n",
    "            y=element['label']\n",
    "        )\n",
    "\n",
    "    return dataset.repeat(NUM_EPOCHS).shuffle(100, seed=seed).batch(BATCH_SIZE).map(batch_format_fn)\n",
    "\n",
    "# Construct a list of datasets (one for each client) from the complete dataset and the number of \n",
    "# clients (it will select the first client ids for simulation).\n",
    "def make_federated_data(client_data, n_clients):    \n",
    "    return [\n",
    "        preprocess(client_data.create_tf_dataset_for_client(x)) # Call previous preprocess method\n",
    "        for x in client_data.client_ids[0:n_clients]\n",
    "    ]\n",
    "\n",
    "# Create the federated train data from the full mnist_train data, and filtering only \n",
    "# NUM_CLIENTS clients\n",
    "train_data = make_federated_data(mnist_train, NUM_CLIENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c6b672-e0ee-483d-ad97-9fd000b7fa7e",
   "metadata": {},
   "source": [
    "## Create a Deep Learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59894fb5-78fd-46a1-829d-95a87ccd4437",
   "metadata": {},
   "source": [
    "For a fair comparison with the rest of frameworks, here we propose two different network architectures: one with a CNN layer, which are widely used for image classification, and another one with only dense layers.\n",
    "\n",
    "Although these architectures are used here, note that any other network architecture supported by keras can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd753053-de53-4d90-b924-09526897b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method that creates a keras model with a CNN\n",
    "def create_keras_CNN():\n",
    "    model = models.Sequential([\n",
    "        layers.Reshape((28, 28, 1), input_shape=(28, 28)),\n",
    "        layers.Conv2D(32, kernel_size=(5, 5), activation=\"relu\", padding=\"same\", strides=1),\n",
    "        layers.MaxPooling2D(pool_size=2, strides=2, padding='valid'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ])\n",
    "        \n",
    "    return model\n",
    "\n",
    "# Method that creates a keras model with only dense layers\n",
    "def create_keras_Dense():\n",
    "    model = models.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28, 1)),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(10, activation=\"softmax\")\n",
    "\n",
    "    ])\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44b20376-dc33-45e5-bc0b-0cf12d3a63a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    # We _must_ create a new model here, and _not_ capture it from an external scope\n",
    "    # TFF will call this within different graph contexts.\n",
    "    \n",
    "    # Comment/uncomment the next lines to create the chosen network architecture\n",
    "    keras_model = create_keras_CNN()\n",
    "    # keras_model = create_keras_Dense()\n",
    "    \n",
    "    return tff.learning.from_keras_model(\n",
    "        keras_model,\n",
    "        input_spec=train_data[0].element_spec,\n",
    "        loss=losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[metrics.SparseCategoricalAccuracy()]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194a3af8-f80c-4064-8810-dffcc938d638",
   "metadata": {},
   "source": [
    "## Training in the federated scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64acc513-ed9b-4007-a1b1-4909daad26c4",
   "metadata": {},
   "source": [
    "Train with weighted FedAvg algorithm.\n",
    "We define the model to use, as well as the optimizer for the clients and server (in both, we are using Adam but with different learning rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "538836ae-e625-4cbb-a4f7-f4b885556977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jose/venv/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jose/venv/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "training_process = build_unweighted_fed_avg(\n",
    "    model_fn,\n",
    "    client_optimizer_fn=lambda: optimizers.Adam(learning_rate=0.001),\n",
    "    server_optimizer_fn=lambda: optimizers.Adam(learning_rate=0.01)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f07be2-befa-446a-a27b-268fc99c91da",
   "metadata": {},
   "source": [
    "Initialize the training process and run it for NUM_ROUNDS rounds of federated learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13c94c2f-8928-4e72-b6fe-bdbe997559bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round  1,  \t Loss=0.2042, \t Accuracy=0.9406\n",
      "Round  2,  \t Loss=0.1533, \t Accuracy=0.9579\n",
      "Round  3,  \t Loss=0.1205, \t Accuracy=0.9675\n",
      "Round  4,  \t Loss=0.0978, \t Accuracy=0.9736\n",
      "Round  5,  \t Loss=0.0817, \t Accuracy=0.9780\n",
      "Round  6,  \t Loss=0.0699, \t Accuracy=0.9811\n",
      "Round  7,  \t Loss=0.0608, \t Accuracy=0.9837\n",
      "Round  8,  \t Loss=0.0535, \t Accuracy=0.9857\n",
      "Round  9,  \t Loss=0.0475, \t Accuracy=0.9873\n",
      "Round 10,  \t Loss=0.0425, \t Accuracy=0.9887\n"
     ]
    }
   ],
   "source": [
    "train_state = training_process.initialize()\n",
    "\n",
    "for round_num in range(1, NUM_ROUNDS+1):\n",
    "    # Train next round (send model to clients, local training, and server model averaging)\n",
    "    result = training_process.next(train_state, train_data)\n",
    "    \n",
    "    # Current state of the model\n",
    "    train_state = result.state\n",
    "    \n",
    "    # Get and print metrics, as the loss and accuracy (averaged across all clients)\n",
    "    train_metrics = result.metrics['client_work']['train']\n",
    "    print('Round {:2d},  \\t Loss={:.4f}, \\t Accuracy={:.4f}'.format(round_num, train_metrics['loss'], \n",
    "                                                                    train_metrics['sparse_categorical_accuracy']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b275eb-e70f-4a4b-a4ea-8ede74570ffa",
   "metadata": {},
   "source": [
    "## Evaluation with test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce167e4b-e27a-446a-b2db-cde0320a3d34",
   "metadata": {},
   "source": [
    "Prepare the model to pass it unseen test data to evaluate its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95f1e69b-def1-4c02-bdc1-b29c4356aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate that the model arquitecture is the one proposed before\n",
    "evaluation_process = build_fed_eval(model_fn)\n",
    "\n",
    "# Initialize the process and set the weights to those previously trained (getting from the training\n",
    "# state and setting to the evaluation one).\n",
    "evaluation_state = evaluation_process.initialize()\n",
    "model_weights = training_process.get_model_weights(train_state)\n",
    "evaluation_state = evaluation_process.set_model_weights(evaluation_state, model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f7d364-6c91-4fb1-83ac-9cfed333f4ab",
   "metadata": {},
   "source": [
    "Prepare the test data as we did with the training one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfaf2718-e4b1-4d51-97b5-e924cb6194c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = make_federated_data(mnist_test, NUM_CLIENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9241091-5c40-468c-973e-bccf6fcbb409",
   "metadata": {},
   "source": [
    "Evaluate the model with test data and print the desired evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3fa7759-5815-4217-b000-fcc36cbd7f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data, \t Loss=0.0821, \t Accuracy=0.9758\n"
     ]
    }
   ],
   "source": [
    "# Pass test data to the model in each client\n",
    "evaluation_output = evaluation_process.next(evaluation_state, test_data)\n",
    "\n",
    "# Get and print metrics\n",
    "eval_metrics = evaluation_output.metrics['client_work']['eval']['current_round_metrics']\n",
    "print('Test data, \\t Loss={:.4f}, \\t Accuracy={:.4f}'.format(eval_metrics['loss'], \n",
    "                                                             eval_metrics['sparse_categorical_accuracy']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
