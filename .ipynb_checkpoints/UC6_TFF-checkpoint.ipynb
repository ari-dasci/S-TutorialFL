{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d85c969a-bbe5-4374-879d-ef8ad7c4f690",
   "metadata": {},
   "source": [
    "# **USE CASE 6.** Clustering using k-means in HFL with TFF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e1c316-db26-45e6-add8-2604b6dc6b11",
   "metadata": {},
   "source": [
    "## Required libraries and configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4151af53-0807-4859-8478-1e7d6b0ec26a",
   "metadata": {},
   "source": [
    "Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b000fad2-9ca1-425c-96e7-ab21b62bed47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 13:27:26.450179: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-23 13:27:28.244032: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-23 13:27:28.244164: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-23 13:27:28.244175: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/jose/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow_federated.python.simulation.datasets import emnist\n",
    "from tensorflow_federated.python.learning.algorithms import build_fed_kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f499cef0-0261-47a1-b2a7-e2819e1d1006",
   "metadata": {},
   "source": [
    "Define some parameters for the simulation, such as the number of clients and the number of rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a676bb7f-845a-4ca3-9b17-d33331dbbd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some parameters\n",
    "NUM_CLIENTS = 10 # Number of clients in the federated scenario\n",
    "NUM_ROUNDS = 10 # Number of learning rounds in the federated computation\n",
    "\n",
    "# Define the seed for random numbers\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96891704-dc2a-443e-a67c-9c48c631f8ed",
   "metadata": {},
   "source": [
    "## Loading and preparing the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28bf10d-edd3-4e9a-b5d9-8632dc5f07b9",
   "metadata": {},
   "source": [
    "Load the MNIST dataset from tensorflow federated. In this case, we only use the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c7b1159-5065-4288-b000-52a39819f36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 13:28:07.616002: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-23 13:28:07.616261: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jose/venv/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jose/venv/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST from tfds. Get only train partition.\n",
    "mnist = tfds.load('mnist')['train']\n",
    "\n",
    "# Transform the input from uint8 to float32, since it is a requirement of the model\n",
    "mnist = mnist.map(lambda sample: {'image': tf.cast(sample['image'], tf.float32),\n",
    "                                  'label': sample['label']})\n",
    "\n",
    "# Transform the data to a dataframe and assign random ids\n",
    "mnist_df = tfds.as_dataframe(mnist)\n",
    "ids = [i for i in range(NUM_CLIENTS) for _ in range(len(mnist)//NUM_CLIENTS)]\n",
    "random.Random(seed).shuffle(ids)\n",
    "mnist_df['id'] = ids\n",
    "\n",
    "# This method receives a client_id, and returns the tf.data.Dataset for that client\n",
    "def create_tf_dataset_for_client_fn(client_id):\n",
    "    client_data = mnist_df[mnist_df['id'] == client_id].drop(columns='id')\n",
    "    return tf.data.Dataset.from_tensor_slices(client_data.to_dict('list'))\n",
    "\n",
    "mnist = tff.simulation.datasets.ClientData.from_clients_and_tf_fn(\n",
    "    client_ids=list(range(0,NUM_CLIENTS)),\n",
    "    serializable_dataset_fn=create_tf_dataset_for_client_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a863e14a-1769-4564-a812-d17e79c28d53",
   "metadata": {},
   "source": [
    "Shuffle and reshape the data into a flat array, and prepare the data of each client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e255d668-d0c3-44f8-a662-44d8dbc1e7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "    def format_fn(element):\n",
    "        return (tf.reshape(element['image'], [784]))\n",
    "\n",
    "    return dataset.shuffle(100, seed=seed).map(format_fn)\n",
    "\n",
    "# Preprocess and get a list of datasets, one for each client\n",
    "client_data = [preprocess(mnist.create_tf_dataset_for_client(x))\n",
    "               for x in mnist.client_ids[0:NUM_CLIENTS]\n",
    "              ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194a3af8-f80c-4064-8810-dffcc938d638",
   "metadata": {},
   "source": [
    "## Building kmeans in the federated scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64acc513-ed9b-4007-a1b1-4909daad26c4",
   "metadata": {},
   "source": [
    "Build kmeans indicating the number of clusters, the data shape, and optionally a tuple of random seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "538836ae-e625-4cbb-a4f7-f4b885556977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans \n",
    "fed_process = build_fed_kmeans(\n",
    "    num_clusters = 10,\n",
    "    random_seed = (seed, seed+1),\n",
    "    data_shape = (784,)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f07be2-befa-446a-a27b-268fc99c91da",
   "metadata": {},
   "source": [
    "Initialize the federated process and run kmeans for NUM_ROUNDS rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1df44df3-02e1-470e-864f-507965333616",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_state = fed_process.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98954320-2446-4a4b-9cdb-5e5bdabb046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for round_num in range(1, NUM_ROUNDS+1):\n",
    "    # Train next round (send model to clients, local execution, and server model averaging)\n",
    "    result = fed_process.next(fed_state, client_data)\n",
    "    \n",
    "    # Current state of the model\n",
    "    fed_state = result.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853faaa4-ed8f-4463-9feb-922ea62a4994",
   "metadata": {},
   "source": [
    "Let's print some results, as the number of data points assigned to each cluster, and the different cluster centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53ef5ee7-484f-4848-822b-6cadb424ea40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assignation of data examples to each cluster\n",
      "[ 55094  36729 139743  68711  47486  51657  65040  23503  51241  60806]\n",
      "\n",
      "Centroids:\n",
      "\t c0: [3.0700319e-06 1.1716238e-05 9.4955603e-06 ...  7.638804e-06 -4.758208e-05  6.353682e-06]\n",
      "\t c1: [-6.0597627e-05  3.8375314e-05 -6.3078851e-06 ...  1.8289855e-05 -1.7047128e-05  2.8148052e-05]\n",
      "\t c2: [-4.040224e-06  1.102012e-05 -3.484598e-06 ... -8.552833e-06 -5.938371e-06 -2.491736e-06]\n",
      "\t c3: [-2.9181876e-06 -1.0620230e-06 -2.6456198e-07 ...  1.1184980e-05 -3.0761712e-05  6.4156325e-06]\n",
      "\t c4: [-4.2209176e-05  6.1375329e-07  2.6055827e-06 ... -8.9833912e-07 -3.4075587e-05  3.6425776e-05]\n",
      "\t c5: [ 2.06467248e-05 -1.32299665e-05 -1.27021030e-05 ...  1.6132815e-05  9.3937109e-07 -8.9756668e-06]\n",
      "\t c6: [ 1.4344982e-05 -2.9590456e-05  2.3032255e-05 ...  1.6019998e-05  1.8676565e-05 -2.3828874e-05]\n",
      "\t c7: [ 8.6653759e-05 -4.4342989e-05 -2.2096896e-05 ... 6.183646e-05 4.678077e-07 6.491635e-05]\n",
      "\t c8: [-2.6130208e-05 -8.6667178e-06 -5.8681912e-06 ...  6.3931529e-06 -3.1591222e-05  5.0537365e-06]\n",
      "\t c9: [-8.3576224e-06  2.1312906e-05  5.8472106e-06 ...  7.8779776e-06  2.8003247e-05 -4.5115225e-06]\n"
     ]
    }
   ],
   "source": [
    "print('Assignation of data examples to each cluster')\n",
    "print(result.state.finalizer)\n",
    "print()\n",
    "print('Centroids:')\n",
    "\n",
    "i=0\n",
    "for c in result.state.global_model_weights:\n",
    "    print('\\t c' + str(i) + ': [' + str(c[0:3])[1:-1] + ' ... ' + str(c[-4:-1])[1:-1] + ']')\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
