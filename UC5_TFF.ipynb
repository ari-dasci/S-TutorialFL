{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d85c969a-bbe5-4374-879d-ef8ad7c4f690",
   "metadata": {},
   "source": [
    "# **USE CASE 5.** Introducing Differential Privacy (in image classification) in TFF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e1c316-db26-45e6-add8-2604b6dc6b11",
   "metadata": {},
   "source": [
    "## Required libraries and configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4151af53-0807-4859-8478-1e7d6b0ec26a",
   "metadata": {},
   "source": [
    "Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b000fad2-9ca1-425c-96e7-ab21b62bed47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 17:04:15.760326: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-20 17:04:18.041411: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-20 17:04:18.041547: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-20 17:04:18.041557: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/jose/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow_federated.python.simulation.datasets import emnist\n",
    "from tensorflow_federated.python.learning.algorithms import build_unweighted_fed_avg, build_fed_eval\n",
    "from tensorflow_federated.python.learning.model_update_aggregator import dp_aggregator\n",
    "from tensorflow.keras import models, layers, losses, metrics, optimizers\n",
    "\n",
    "# Option for debugging warning errors\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f499cef0-0261-47a1-b2a7-e2819e1d1006",
   "metadata": {},
   "source": [
    "Define some parameters for the simulation, such as the number of clients in the federated scenario, the number of federated rounds, the number of epochs of each client before communicating, and the batch size for training phase. Besides, the DP_MULTIPLIERS comprises the different multipliers to evaluate for the gaussian noise in the DP mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a676bb7f-845a-4ca3-9b17-d33331dbbd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some parameters\n",
    "NUM_CLIENTS = 10 # Number of clients in the federated scenario\n",
    "NUM_ROUNDS = 10 # Number of learning rounds in the federated computation\n",
    "NUM_EPOCHS = 5 # Number of epochs that the local dataset is seen each round\n",
    "BATCH_SIZE = 20 # Batch size for training phase\n",
    "\n",
    "DP_MULTIPLIERS = [0.0, 0.05, 0.1, 0.2, 0.5] # Gaussian noise multipliers for DP mechanism\n",
    "\n",
    "# Define the seed for random numbers\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.keras.utils.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96891704-dc2a-443e-a67c-9c48c631f8ed",
   "metadata": {},
   "source": [
    "## Loading and preparing the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8e290d-ffb1-4712-a03a-118485348c2e",
   "metadata": {},
   "source": [
    "For a more detailed information about the data loading, please see use case 1.1 in TFF.\n",
    "\n",
    "The `iid_data` variable indicates, if True, that the i.i.d. MNIST dataset is i.i.d. partitioned; otherwise, the non-i.i.d. partition is obtained from TFF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c7b1159-5065-4288-b000-52a39819f36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 17:04:24.621840: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-20 17:04:24.622423: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "iid_data = False\n",
    "\n",
    "if iid_data:\n",
    "    # Load MNIST from tfds, and get train and test partitions\n",
    "    mnist = tfds.load('mnist')\n",
    "    mnist_train, mnist_test = mnist['train'], mnist['test']\n",
    "\n",
    "    # Transform the data to a dataframe\n",
    "    mnist_train_df = tfds.as_dataframe(mnist_train)\n",
    "\n",
    "    # Create a random list of ids and assign to the dataframe\n",
    "    ids_train = [i for i in range(NUM_CLIENTS) for _ in range(len(mnist_train)//NUM_CLIENTS)]\n",
    "    random.Random(seed).shuffle(ids_train)\n",
    "    mnist_train_df['id'] = ids_train\n",
    "\n",
    "    # Do the same with the test data\n",
    "    mnist_test_df = tfds.as_dataframe(mnist_test) \n",
    "    ids_test = [i for i in range(NUM_CLIENTS) for _ in range(len(mnist_test)//NUM_CLIENTS)]\n",
    "    random.Random(seed+1).shuffle(ids_test)\n",
    "    mnist_test_df['id'] = ids_test\n",
    "\n",
    "    # This method receives a client_id, and returns the training tf.data.Dataset for that client\n",
    "    def create_tf_dataset_for_client_fn_train(client_id):\n",
    "        client_data = mnist_train_df[mnist_train_df['id'] == client_id].drop(columns='id')\n",
    "        return tf.data.Dataset.from_tensor_slices(client_data.to_dict('list'))\n",
    "\n",
    "    # This method receives a client_id, and returns the testing tf.data.Dataset for that client\n",
    "    def create_tf_dataset_for_client_fn_test(client_id):\n",
    "        client_data = mnist_test_df[mnist_test_df['id'] == client_id].drop(columns='id')\n",
    "        return tf.data.Dataset.from_tensor_slices(client_data.to_dict('list'))\n",
    "\n",
    "    mnist_train = tff.simulation.datasets.ClientData.from_clients_and_tf_fn(\n",
    "        client_ids=list(range(0,NUM_CLIENTS)),\n",
    "        serializable_dataset_fn=create_tf_dataset_for_client_fn_train\n",
    "    )\n",
    "    mnist_test = tff.simulation.datasets.ClientData.from_clients_and_tf_fn(\n",
    "        client_ids=list(range(0,NUM_CLIENTS)),\n",
    "        serializable_dataset_fn=create_tf_dataset_for_client_fn_test\n",
    "    )\n",
    "else:\n",
    "    # Load federated version of MNIST\n",
    "    mnist_train, mnist_test = emnist.load_data(only_digits=True)\n",
    "\n",
    "# Preprocess the dataset as a OrderedDict\n",
    "def preprocess(dataset):\n",
    "    def batch_format_fn(element):\n",
    "        if iid_data:\n",
    "            return collections.OrderedDict(\n",
    "                x=element['image']/255,\n",
    "                y=element['label']\n",
    "            )\n",
    "        else:\n",
    "            return collections.OrderedDict(\n",
    "                x=element['pixels'],\n",
    "                y=element['label']\n",
    "            )\n",
    "\n",
    "    return dataset.repeat(NUM_EPOCHS).shuffle(100, seed=seed).batch(BATCH_SIZE).map(batch_format_fn)\n",
    "\n",
    "# Construct a list of datasets (one for each client) from the complete dataset and the number of \n",
    "# clients (it will select the first client ids for simulation).\n",
    "def make_federated_data(client_data, n_clients):    \n",
    "    return [\n",
    "        preprocess(client_data.create_tf_dataset_for_client(x)) # Call previous preprocess method\n",
    "        for x in client_data.client_ids[0:n_clients]\n",
    "    ]\n",
    "\n",
    "# Create the federated train and testing data\n",
    "train_data = make_federated_data(mnist_train, NUM_CLIENTS)\n",
    "test_data = make_federated_data(mnist_test, NUM_CLIENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c6b672-e0ee-483d-ad97-9fd000b7fa7e",
   "metadata": {},
   "source": [
    "## Create a Deep Learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59894fb5-78fd-46a1-829d-95a87ccd4437",
   "metadata": {},
   "source": [
    "Define a CNN model with keras. \n",
    "Note that any network architecture supported by keras can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd753053-de53-4d90-b924-09526897b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Reshape((28, 28, 1), input_shape=(28, 28)),\n",
    "        layers.Conv2D(32, kernel_size=(5, 5), activation=\"relu\", padding=\"same\", strides=1),\n",
    "        layers.MaxPooling2D(pool_size=2, strides=2, padding='valid'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ])\n",
    "        \n",
    "    return model\n",
    "\n",
    "def model_fn():\n",
    "    keras_model = create_keras_model()\n",
    "    \n",
    "    return tff.learning.from_keras_model(\n",
    "        keras_model,\n",
    "        input_spec=train_data[0].element_spec,\n",
    "        loss=losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[metrics.SparseCategoricalAccuracy()]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194a3af8-f80c-4064-8810-dffcc938d638",
   "metadata": {},
   "source": [
    "## Training (and evaluating) federated model with Differential Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64acc513-ed9b-4007-a1b1-4909daad26c4",
   "metadata": {},
   "source": [
    "For training a model in a federated learning framework using differential privacy, we use the `dp_aggregator` provided by TFF, along with the unweighted FedAvg aggregator. The rest of the process is similar to the one performed in use case 1.1 in TFF.\n",
    "\n",
    "We define a method to train and test the model with a given value for the gaussian noise multiplier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "538836ae-e625-4cbb-a4f7-f4b885556977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fl_train_test_with_DP(noise_multiplier, train_data, test_data):\n",
    "    print('m: ' + str(noise_multiplier))\n",
    "\n",
    "    # Aggregator with Differential Privacy\n",
    "    aggregation_factory = dp_aggregator(noise_multiplier, NUM_CLIENTS)\n",
    "\n",
    "    training_process = build_unweighted_fed_avg(\n",
    "        model_fn,\n",
    "        client_optimizer_fn=lambda: optimizers.Adam(learning_rate=0.001),\n",
    "        server_optimizer_fn=lambda: optimizers.Adam(learning_rate=0.01),\n",
    "        model_aggregator=aggregation_factory\n",
    "    )\n",
    "    \n",
    "    # The rest of the code below in this cell is the same as in Use Case 1.1\n",
    "    \n",
    "    # Training\n",
    "    train_state = training_process.initialize()\n",
    "\n",
    "    for round_num in range(1, NUM_ROUNDS+1):\n",
    "        # Train next round (send model to clients, local training, and server model averaging)\n",
    "        result = training_process.next(train_state, train_data)\n",
    "\n",
    "        # Current state of the model\n",
    "        train_state = result.state\n",
    "\n",
    "        # Get and print metrics, as the loss and accuracy (averaged across all clients)\n",
    "        train_metrics = result.metrics['client_work']['train']\n",
    "        print('Round {:2d},  \\t Loss={:.4f}, \\t Accuracy={:.4f}'.format(round_num, train_metrics['loss'], train_metrics['sparse_categorical_accuracy']))\n",
    "        \n",
    "    # Evaluation\n",
    "    # Indicate that the model arquitecture is the one proposed before\n",
    "    evaluation_process = build_fed_eval(model_fn)\n",
    "\n",
    "    # Initialize the process and set the weights to those previously trained (getting from the training state and setting to the evaluation one).\n",
    "    evaluation_state = evaluation_process.initialize()\n",
    "    model_weights = training_process.get_model_weights(train_state)\n",
    "    evaluation_state = evaluation_process.set_model_weights(evaluation_state, model_weights)\n",
    "    \n",
    "    # Pass test data to the model in each client\n",
    "    evaluation_output = evaluation_process.next(evaluation_state, test_data)\n",
    "\n",
    "    # Get and print metrics\n",
    "    eval_metrics = evaluation_output.metrics['client_work']['eval']['current_round_metrics']  \n",
    "    print('Test data, \\t Loss={:.4f}, \\t Accuracy={:.4f}'.format(eval_metrics['loss'], eval_metrics['sparse_categorical_accuracy']))\n",
    "\n",
    "    print('---\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6029a6-da6b-4c68-b736-094c7f3f09dc",
   "metadata": {},
   "source": [
    "Try with different multipliers for the DP process, and compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98954320-2446-4a4b-9cdb-5e5bdabb046f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m: 0.0\n",
      "WARNING:tensorflow:From /home/jose/venv/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jose/venv/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round  1,  \t Loss=2.2077, \t Accuracy=0.2379\n",
      "Round  2,  \t Loss=2.0795, \t Accuracy=0.3981\n",
      "Round  3,  \t Loss=1.7940, \t Accuracy=0.5813\n",
      "Round  4,  \t Loss=1.5610, \t Accuracy=0.6728\n",
      "Round  5,  \t Loss=1.3017, \t Accuracy=0.7603\n",
      "Round  6,  \t Loss=1.0646, \t Accuracy=0.8152\n",
      "Round  7,  \t Loss=0.8369, \t Accuracy=0.8582\n",
      "Round  8,  \t Loss=0.6483, \t Accuracy=0.8842\n",
      "Round  9,  \t Loss=0.5184, \t Accuracy=0.8967\n",
      "Round 10,  \t Loss=0.4202, \t Accuracy=0.9053\n",
      "Test data, \t Loss=0.6447, \t Accuracy=0.8017\n",
      "Training and testing in 33.12 seconds\n",
      "---\n",
      "\n",
      "\n",
      "m: 0.05\n",
      "Round  1,  \t Loss=2.2136, \t Accuracy=0.2469\n",
      "Round  2,  \t Loss=2.0113, \t Accuracy=0.4535\n",
      "Round  3,  \t Loss=1.8386, \t Accuracy=0.5953\n",
      "Round  4,  \t Loss=1.6572, \t Accuracy=0.6792\n",
      "Round  5,  \t Loss=1.4541, \t Accuracy=0.7484\n",
      "Round  6,  \t Loss=1.2471, \t Accuracy=0.7988\n",
      "Round  7,  \t Loss=1.0507, \t Accuracy=0.8325\n",
      "Round  8,  \t Loss=0.8802, \t Accuracy=0.8572\n",
      "Round  9,  \t Loss=0.7283, \t Accuracy=0.8794\n",
      "Round 10,  \t Loss=0.6034, \t Accuracy=0.8887\n",
      "Test data, \t Loss=0.7302, \t Accuracy=0.8103\n",
      "Training and testing in 29.55 seconds\n",
      "---\n",
      "\n",
      "\n",
      "m: 0.1\n",
      "Round  1,  \t Loss=2.2201, \t Accuracy=0.2298\n",
      "Round  2,  \t Loss=2.0840, \t Accuracy=0.3714\n",
      "Round  3,  \t Loss=1.9616, \t Accuracy=0.5037\n",
      "Round  4,  \t Loss=1.8340, \t Accuracy=0.5877\n",
      "Round  5,  \t Loss=1.7123, \t Accuracy=0.6560\n",
      "Round  6,  \t Loss=1.5762, \t Accuracy=0.7119\n",
      "Round  7,  \t Loss=1.4323, \t Accuracy=0.7558\n",
      "Round  8,  \t Loss=1.2917, \t Accuracy=0.7883\n",
      "Round  9,  \t Loss=1.1562, \t Accuracy=0.8156\n",
      "Round 10,  \t Loss=1.0242, \t Accuracy=0.8389\n",
      "Test data, \t Loss=1.1765, \t Accuracy=0.7845\n",
      "Training and testing in 30.25 seconds\n",
      "---\n",
      "\n",
      "\n",
      "m: 0.2\n",
      "Round  1,  \t Loss=2.2718, \t Accuracy=0.2051\n",
      "Round  2,  \t Loss=2.1611, \t Accuracy=0.3043\n",
      "Round  3,  \t Loss=2.0682, \t Accuracy=0.4068\n",
      "Round  4,  \t Loss=1.9812, \t Accuracy=0.4788\n",
      "Round  5,  \t Loss=1.8948, \t Accuracy=0.5387\n",
      "Round  6,  \t Loss=1.8252, \t Accuracy=0.5726\n",
      "Round  7,  \t Loss=1.7544, \t Accuracy=0.6064\n",
      "Round  8,  \t Loss=1.6848, \t Accuracy=0.6305\n",
      "Round  9,  \t Loss=1.6089, \t Accuracy=0.6636\n",
      "Round 10,  \t Loss=1.5307, \t Accuracy=0.6918\n",
      "Test data, \t Loss=1.8241, \t Accuracy=0.5345\n",
      "Training and testing in 30.67 seconds\n",
      "---\n",
      "\n",
      "\n",
      "m: 0.5\n",
      "Round  1,  \t Loss=2.2299, \t Accuracy=0.2305\n",
      "Round  2,  \t Loss=2.2170, \t Accuracy=0.2514\n",
      "Round  3,  \t Loss=2.1808, \t Accuracy=0.3027\n",
      "Round  4,  \t Loss=2.1471, \t Accuracy=0.3292\n",
      "Round  5,  \t Loss=2.1206, \t Accuracy=0.3601\n",
      "Round  6,  \t Loss=2.0895, \t Accuracy=0.3920\n",
      "Round  7,  \t Loss=2.0636, \t Accuracy=0.4041\n",
      "Round  8,  \t Loss=2.0415, \t Accuracy=0.4183\n",
      "Round  9,  \t Loss=2.0110, \t Accuracy=0.4393\n",
      "Round 10,  \t Loss=1.9850, \t Accuracy=0.4504\n",
      "Test data, \t Loss=2.2028, \t Accuracy=0.1552\n",
      "Training and testing in 30.53 seconds\n",
      "---\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = DP_MULTIPLIERS[0]\n",
    "\n",
    "for m in DP_MULTIPLIERS:\n",
    "    fl_train_test_with_DP(m, train_data, test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
